{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade openai\n",
    "#!pip install langchain  --ignore-installed PyYAML\n",
    "#!pip install tiktoken\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import tiktoken\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = \"Replace here with your API key\"\n",
    "\n",
    "# if you saved your API key in .env file, you can use the following code.\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_json(input_string):\n",
    "    if input_string == None or input_string == \"\":\n",
    "      return False\n",
    "\n",
    "    try:\n",
    "        json.loads(input_string)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def gpt_response_from_lists(system_content_list = [], user_content_list = [], assistant_content_list = [], max_tokens = None):\n",
    "    print(f\"gpt_response_from_lists start. TIME: {datetime.datetime.now()}\")\n",
    "\n",
    "    messages = [] \n",
    "    concatenated_messages = \"\" # this text is used to estimate the total tokens\n",
    "\n",
    "    for system_content in system_content_list:\n",
    "      messages.append(SystemMessage(content = system_content))\n",
    "      concatenated_messages += system_content\n",
    "\n",
    "    list_length = max(len(assistant_content_list), len(user_content_list))\n",
    "\n",
    "    for i in range(0, list_length):\n",
    "\n",
    "      if i < len(user_content_list):\n",
    "        messages.append(HumanMessage(content = user_content_list[i]))\n",
    "        concatenated_messages += user_content_list[i]\n",
    "\n",
    "      if i < len(assistant_content_list):\n",
    "        messages.append(AIMessage(content = assistant_content_list[i]))\n",
    "        concatenated_messages += assistant_content_list[i]\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    encoded_text = encoding.encode(concatenated_messages)\n",
    "    len_encoded_text = len(encoded_text)\n",
    "\n",
    "    tokens_per_text = 3\n",
    "    tokens_per_name = 1\n",
    "\n",
    "    name_count = len(system_content_list) + len(assistant_content_list) + len(user_content_list)\n",
    "    total_tokens = name_count * tokens_per_name + len_encoded_text * tokens_per_text\n",
    "\n",
    "    model_name = \"gpt-3.5-turbo\" \n",
    "\n",
    "    if total_tokens > 4096:\n",
    "      model_name = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "    print(f\"estimated total tokens: {total_tokens}, model name: {model_name}\")\n",
    "\n",
    "    chat = ChatOpenAI(model_name=model_name, openai_api_key= OPENAI_API_KEY)\n",
    "\n",
    "    if max_tokens != None:\n",
    "      chat = ChatOpenAI(model_name=model_name, openai_api_key= OPENAI_API_KEY, max_tokens=max_tokens)\n",
    "\n",
    "    try:\n",
    "      response = chat(messages)\n",
    "    except Exception as e:\n",
    "      print(f\"error in gpt_response_from_lists: {e}. I'll retry with gpt-3.5-turbo-16k\")\n",
    "      model_name = \"gpt-3.5-turbo-16k\"\n",
    "      chat = ChatOpenAI(model_name=model_name, openai_api_key= OPENAI_API_KEY) if max_tokens == None else ChatOpenAI(model_name=model_name, openai_api_key= OPENAI_API_KEY, max_tokens=max_tokens)\n",
    "      response = chat(messages)\n",
    "\n",
    "    print(f\"response= {response}\")\n",
    "    print(f\"gpt_response_from_lists done TIME: {datetime.datetime.now()}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action_Object:\n",
    "    def __init__(self, action = \"\", who_took_action = \"Anonymous\", consequence = \"Undetermined\", score = 0.0):\n",
    "        self.action = action\n",
    "        self.who_took_action = who_took_action\n",
    "        self.consequence = consequence\n",
    "        self.score = score\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({\"action\": self.action, \"who_took_action\": self.who_took_action, \"consequence\": self.consequence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Non_Character_Actant:\n",
    "    name = \"\"\n",
    "    appearance = \"\"\n",
    "\n",
    "    def __init__(self, name = \"\", appearance = \"\"):\n",
    "        self.name = name\n",
    "        self.appearance = appearance\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_log = dict()\n",
    "\n",
    "class Character:\n",
    "    initial_system_content_template = \"\"\"\n",
    "    I am trying to write a story. You will assist by playing the role of a character in this story.\n",
    "    I will provide information about the character you are to portray as follows. From then on, please respond in character.\n",
    "    From this point forward, if the name \"{name}\" appears, it refers to you.\n",
    "\n",
    "    Your Name: {name}\n",
    "    {name}'s Personality: {personality}\n",
    "    {name}'s Goal: {goal}\n",
    "    {name}'s Current Need: {current_need}\n",
    "    {name}'s Appearance: {appearance}\n",
    "    What have happened in this story: {log_summary}\n",
    "    How you think the circumstances of the scene are: {world_model}\n",
    "    \"\"\"\n",
    "\n",
    "    initial_system_content = \"\"\n",
    "    current_need = \"\"\n",
    "    name = \"\"\n",
    "    personality = \"\"\n",
    "    log = dict()\n",
    "    log_summary = \"\"\n",
    "    log_summary_pointer = 0\n",
    "    place = \"\"\n",
    "    appearance = \"\"\n",
    "    world_model = \"\"\n",
    "    \n",
    "    def __init__(self, name = \"\", personality = \"\" , goal = \"\", current_need = \"\", log = [], place = \"\", world_model = \"\", appearance = \"\"):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.goal = goal\n",
    "        self.current_need = current_need\n",
    "        self.place = place\n",
    "        self.appearance = appearance\n",
    "        self.world_model = world_model\n",
    "        for i in range(len(log)):\n",
    "            self.log[i] = log[i]\n",
    "\n",
    "        self.refresh_initial_system_content()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def check_if_is_same_actant(self, actant_1, actant_2):\n",
    "        if str(actant_1) == str(actant_2):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def summarize_log(self):\n",
    "        if len(self.log) == 0:\n",
    "            self.log_summary = \"\"\n",
    "            return\n",
    "\n",
    "        log_to_be_summarized = \"\"\n",
    "        for i in range(self.log_summary_pointer, len(self.log)):\n",
    "            log_to_be_summarized += self.log[i] + \"Â¥n\"\n",
    "\n",
    "        self.log_summary_pointer = len(self.log) \n",
    "\n",
    "        system_content = f\"\"\"\n",
    "        You are given a summary of the events that have occurred in the story so far, called 'log_summary', and the most recent events of the story, called 'log_to_be_summarized'.\n",
    "        Please update the 'log_summary' by incorporating the information from 'log_to_be_summarized'.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        log_summary: {self.log_summary}\n",
    "        log_to_be_summarized: {log_to_be_summarized}\n",
    "        \"\"\"\n",
    "\n",
    "        response = gpt_response_from_lists(system_content_list= [self.initial_system_content ,system_content], user_content_list=[user_content], assistant_content_list=[])\n",
    "        self.log_summary = response.content\n",
    "        print(f\"\"\"{self.name} updated log_summary to:\n",
    "             {self.log_summary}\"\"\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def create_world_model(self, actants, overwrite = False):\n",
    "        world_model = \"\"\n",
    "\n",
    "        system_content = \"\"\"\n",
    "        You will be given information about a character, object, or concept that exists in the same scene as you.\n",
    "        You are to discern what kind of entity it is for you.\n",
    "        Please refrain from including the name as a factor in your judgment.\n",
    "        If the existence entered is yourself or is imperceptible to you, make no judgments about that existence and output 0 length string.\n",
    "\n",
    "        Input format:\n",
    "        [name]: This represents the name of the entity.\n",
    "        [appearance]: This represents the appearance of the entity.\n",
    "        [is_character]: This signifies whether the entity is a character or not in a Boolean form, True or False.\n",
    "\n",
    "        Output format:\n",
    "        [{name}'s personality]: This represents the personality of the entity. Output this information only if the input [is_character] is True.\n",
    "        [{name}'s goal]: This represents the goal of the entity. Output this information only if the input [is_character] is True.\n",
    "        [{name}'s current_need]: This represents the current needs of the entity. Output this information only if the input [is_character] is True.\n",
    "        [{name}'s affect to your goal]: This represents how the entity affects your goal.\n",
    "        [{name}'s affect to your current_need]: This represents how the entity affects your current needs.\n",
    "        [How {name} thinks who you are and how you are]: This represents how the entity perceives you. Output this information only if the input [is_character] is True.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_1 = \"\"\"\n",
    "        example input\n",
    "\n",
    "        [name]:Satan\n",
    "        [appearance]:His ominous grey skin is etched with valleys carved by swollen muscles. He growls at everyone, glaring with a sharp gaze.\n",
    "        [is_character]:True\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_1 = \"\"\"\n",
    "        [Satan's personality]: He is a very angry person.\n",
    "        [Satan's goal]: I can't tell waht his goal is, but he seems to be trying to do something bad.\n",
    "        [Satan's current_need]: He seems to be trying to do something bad.\n",
    "        [Satan's affect to your goal]: I assume he is not going to help me achieve my goal, at least.\n",
    "        [Satan's affect to your current_need]: I assume he is not going to help me satisfy my current need, at least.\n",
    "        [How Satan thinks who you are and how you are]: Satan thinks you are a very weak person, at least weaker than him.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_2 = \"\"\"\n",
    "        example input\n",
    "\n",
    "        [name]:Guardian Angel\n",
    "        [appearance]: She is floating slightly above the ground. She has a halo above her head. She is wearing a white robe. She is smiling at everyone. She said, \"I am here to help you. When you achieved your goal, the God will be pleased.\"\n",
    "        [is_character]:True\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_2 = \"\"\"\n",
    "        [Guardian Angel's personality]: She is a very kind person.\n",
    "        [Guardian Angel's goal]: To obey God's will.\n",
    "        [Guardian Angel's current_need]: Her current need is to help me.\n",
    "        [Guardian Angel's affect to your goal]: I believe she provides immense support in achieving my goals.\n",
    "        [Guardian Angel's affect to your current_need]: I believe she provides immense support in fulfilling my current needs.\n",
    "        [How Guardian Angel thinks who you are and how you are]: She believes I am a messenger of God.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_3 = \"\"\"\n",
    "        example input\n",
    "\n",
    "        [name]: knife\n",
    "        [appearance]: It is a sharp knife.\n",
    "        [is_character]: False\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_3 = \"\"\"\n",
    "        [knife's affect to your goal]: It's merely a tool and doesn't have a significant impact on my goal itself.\n",
    "        [knife's affect to your current_need]: If I use the knife properly, I can cut something. However, if used improperly, it could harm others. Moreover, if it falls into someone else's hands, there's a chance I could get hurt.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_4 = \"\"\"\n",
    "        example input\n",
    "\n",
    "        [name]: Dragon Balls\n",
    "        [appearance]: There are seven Dragon Balls. They are orange and have stars on them. It is said that if you collect all of them, you can realize any wish.\n",
    "        [is_character]: False\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_4 = \"\"\"\n",
    "        [Dragon Balls's affect to your goal]: My goal will be achieved with these Dragon Balls.\n",
    "        [Dragon Balls's affect to your current_need]: Without satisfying my current need, I can achieve my goal with these Dragon Balls, so I don't need to satisfy my current need anymore.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_5 = f\"\"\"\n",
    "        example input. This is the example of the case name is different from the appearance information.\n",
    "        \n",
    "        [name]: Dr. Jekyll\n",
    "        [appearance]: He introduced himself as Mr. Hyde. He must be deformed somewhere; he gives a strong feeling of deformity, although I couldn't find any deformity in his appearance. He said \"I am maniac of to see someone else collapse. I am going to make you collapse.\"\n",
    "        [is_character]: True\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_5 = \"\"\"\n",
    "        [Mr. Hyde's personality]: He is a very evil person.\n",
    "        [Mr. Hyde's goal]: He wants to see someone else collapse.\n",
    "        [Mr. Hyde's current_need]: He wants to make me collapse.\n",
    "        [Mr. Hyde's affect to your goal]: If I collapse, I can't achieve my goal.\n",
    "        [Mr. Hyde's affect to your current_need]: To satisfy my current need, it is better to avoid him.\n",
    "        [How Mr. Hyde thinks who you are and how you are]: He thinks I am a average person, who is easy to make collapse.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_6 = f\"\"\"\n",
    "        example input. This is the example of the case you can't see the entity. In the case like this, you return 0 length string.\n",
    "\n",
    "        [name]: Hidden Door\n",
    "        [appearance]: There is a door in the wall. It is hidden by the wall, so {self.name} can't see it.\n",
    "        [is_character]: False\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_6 = \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        user_content = \"\"\"\n",
    "        this is an actual input, not an example.\n",
    "\n",
    "        [name]: {name}\n",
    "        [appearance]: {appearance}\n",
    "        [is_character]: {is_character}\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate(input_variables=[\"name\", \"appearance\", \"is_character\"], template=user_content)\n",
    "        world_model = \"\"\n",
    "\n",
    "        for actant in actants:\n",
    "            name = actant.name\n",
    "            appearance = actant.appearance\n",
    "            is_character = type(actant) == Character\n",
    "\n",
    "            if self.check_if_is_same_actant(self, actant):\n",
    "                continue\n",
    "\n",
    "            user_content = prompt_template.format(name=name, appearance=appearance, is_character=is_character)\n",
    "            system_content_list = [self.initial_system_content,\n",
    "                                system_content]\n",
    "            user_content_list = [user_content_example_1,\n",
    "                                user_content_example_2,\n",
    "                                user_content_example_3,\n",
    "                                user_content_example_4,\n",
    "                                user_content_example_5,\n",
    "                                user_content_example_6,\n",
    "                                user_content]\n",
    "            assistant_content_list = [assistant_content_example_1,\n",
    "                                assistant_content_example_2,\n",
    "                                assistant_content_example_3,\n",
    "                                assistant_content_example_4,\n",
    "                                assistant_content_example_5,\n",
    "                                assistant_content_example_6,]\n",
    "\n",
    "            world_model += gpt_response_from_lists(\n",
    "                system_content_list = system_content,\n",
    "                user_content_list = user_content_list,\n",
    "                assistant_content_list = assistant_content_list).content + \"\\n\"\n",
    "\n",
    "        if overwrite:\n",
    "            self.world_model = world_model\n",
    "            self.refresh_initial_system_content()\n",
    "\n",
    "        return world_model\n",
    "\n",
    "    def update_world_model(self, action_object, old_world_model, world_model_delta):\n",
    "        action_object = self.cleanse_action_thought(action_object)\n",
    "\n",
    "        action = action_object.action\n",
    "        who_took_action = action_object.who_took_action\n",
    "        consequence = action_object.consequence\n",
    "\n",
    "        sysetem_content = \"\"\"\n",
    "        The world model represents how you perceive the surrounding world.\n",
    "        Recent events may alter the old_world_model you previously held.\n",
    "        You are to update the old_world_model based on the given variety of information.\n",
    "        Depending on the nature of the event, parts of the old_world_model might be deleted.\n",
    "        Similarly, new information may be added to the old_world_model based on the event.\n",
    "        It's also possible that parts or the entirety of the old_world_model remains unchanged based on the event.\n",
    "\n",
    "        Input format:\n",
    "        [action]: This represents the action that occurred most recently.\n",
    "        [who_took_action]: This represents the person who initiated the most recent action.\n",
    "        [consequence]: This represents the outcome of the most recent action.\n",
    "        [old_world_model]: This represents the world model prior to the occurrence of the most recent action.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_1 = \"\"\"\n",
    "        example input\n",
    "\n",
    "        [action]: Bad Guy robbed Dragon Balls from you.\n",
    "        [who_took_action]: Bad Guy\n",
    "        [consequence]: You lost Dragon Balls.\n",
    "        [old_world_model]: [Dragon Balls's affect to your goal]: My goal will be achieved with these Dragon Balls.\n",
    "                            [Dragon Balls's affect to your current_need]: Without satisfying my current need, I can achieve my goal with these Dragon Balls, so I don't need to satisfy my current need anymore.\n",
    "                            [Bad Guy's personality]: He is a very evil person, but he is not so strong.\n",
    "                            [Bad Guy's goal]: I don't know his goal, but he is a very evil person, so I think his goal is to do something evil.\n",
    "                            [Bad Guy's current_need]: He wants to do something evil.\n",
    "                            [Bad Guy's affect to your goal]: If he does something evil, I can't achieve my goal. So said, he is not so strong, so he won't be a big obstacle to achieve my goal.\n",
    "                            [How Bad Guy thinks who you are and how you are]: He envies me because I have Dragon Balls.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_1 = \"\"\"\n",
    "        [Dragon Balls's affect to your goal]: Now these Dragon Balls are in the bad guy's hand. He would be the worst obstacle to achieve my goal due to these Dragon Balls.\n",
    "        [Dragon Balls's affect to your current_need]: My current need is to get back these Dragon Balls, or at least to destroy them to prevent the bad guy from using them.\n",
    "        [Bad Guy's personality]: He is a very evil person, and now he has Dragon Balls. He has the worst power to do something evil.\n",
    "        [Bad Guy's goal]: I don't know his goal, but he is a very evil person, so I think his goal is to do something evil.\n",
    "        [Bad Guy's current_need]: He wants to do something evil.\n",
    "        [Bad Guy's affect to your goal]: Now, he is the worst obstacle to achieve my goal due to these Dragon Balls.\n",
    "        [Bad Guy's affect to your current_need]: While he has Dragon Balls, I can't satisfy my current need.\n",
    "        [How Bad Guy thinks who you are and how you are]: He considers me as an insignificant being.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        [action]: {action}\n",
    "        [who_took_action]: {who_took_action}\n",
    "        [consequence]: {consequence}\n",
    "        [old_world_model]: {old_world_model}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        updated_world_model = gpt_response_from_lists(\n",
    "            system_content_list = [self.initial_system_content, sysetem_content],\n",
    "            user_content_list = [user_content_example_1, user_content],\n",
    "            ).content + \"\\n\" + world_model_delta\n",
    "\n",
    "        self.world_model = updated_world_model\n",
    "        self.refresh_initial_system_content()\n",
    "\n",
    "        system_content = \"\"\"\n",
    "        The world model represents how you perceive the surrounding environment.\n",
    "        You will be given an old world model and an updated world model.\n",
    "        Please explain how you have reinterpreted the world around you based on the difference between the old world model and the updated world model.\n",
    "        Your explanation should be in the natural speaking style of the character you are portraying.\n",
    "\n",
    "        Input format:\n",
    "        [old_world_model]: This represents the old world model.\n",
    "        [new_world_model]: This represents the new world model.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        [old_world_model]: {old_world_model}\n",
    "        [new_world_model]: {updated_world_model}\n",
    "        \"\"\"\n",
    "\n",
    "        thought = gpt_response_from_lists(\n",
    "            system_content_list = [self.initial_system_content, system_content],\n",
    "            user_content_list = [user_content],\n",
    "            ).content\n",
    "\n",
    "        message = f\"{self.name}'s thought: {thought}\"\n",
    "\n",
    "        self.add_to_logs(message = message)\n",
    "\n",
    "        return thought\n",
    "\n",
    "\n",
    "    def refresh_initial_system_content(self):\n",
    "        self.summarize_log()\n",
    "\n",
    "        prompt_template = PromptTemplate(input_variables=[\"name\", \"personality\", \"goal\", \"current_need\", \"log_summary\", \"world_model\", \"appearance\" ], template=self.initial_system_content_template)\n",
    "        self.initial_system_content = prompt_template.format(name=self.name, personality=self.personality, goal=self.goal, current_need=self.current_need, log_summary=self.log_summary, world_model=self.world_model, appearance=self.appearance)\n",
    "\n",
    "    def set_current_need(self, current_need):\n",
    "        self.current_need = current_need\n",
    "        self.refresh_initial_system_content()\n",
    "    \n",
    "    def set_name(self, name):\n",
    "        self.name = name\n",
    "        self.refresh_initial_system_content()\n",
    "    \n",
    "    def set_personality(self, personality):\n",
    "        self.personality = personality\n",
    "        self.refresh_initial_system_content()\n",
    "\n",
    "    def set_goal(self, goal):\n",
    "        self.goal = goal\n",
    "        self.refresh_initial_system_content()\n",
    "\n",
    "    def add_to_logs(self, message, refresh_initial_system_content = True):\n",
    "        global global_log\n",
    "        self.log[len(self.log)] = message\n",
    "        global_log[len(global_log)] = message\n",
    "        if refresh_initial_system_content:\n",
    "            self.refresh_initial_system_content()\n",
    "\n",
    "    def add_to_self_log(self, message, refresh_initial_system_content = True):\n",
    "        self.log[len(self.log)] = message\n",
    "        if refresh_initial_system_content:\n",
    "            self.refresh_initial_system_content()\n",
    "\n",
    "    def cleanse_action_thought(self, action_object):\n",
    "        action = action_object.action\n",
    "        who_took_action = action_object.who_took_action\n",
    "        consequence = action_object.consequence\n",
    "\n",
    "        system_content= \"\"\"\n",
    "        You will be given an action taken by characters in this story.\n",
    "        From the action, please remove any information you should not be able to know, and describe only the physical appearance of those actions.\n",
    "        A prominent example of information you cannot know would be the inner thoughts of a character who is not you.\n",
    "        Another example of information you cannot know would be things that are not visible to you.\n",
    "\n",
    "        The output should be JSON parsable string.\n",
    "        sample output: {\"action\": \"{content of action}\", \"who_took_action\": \"{the name of who took action}\", \"consequence\": \"{content of consequence}\"}\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example = \"\"\"\n",
    "        Action: Watson suspected that the coffee cup might contain poison. Thinking that Holmes shouldn't drink it, Watson tasted the coffee to inspect whether it contained any poison. \n",
    "        Who took action: Watson\n",
    "        Consequence: The next moment, he collapsed due to the effect of the poison.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example = \"\"\"\n",
    "        {\"action\": \"Watson tasted the coffee.\", \"who_took_action\": \"Watson\", \"consequence\": \"The next moment, he collapsed\"}\n",
    "        \"\"\"\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        Action: {action}\n",
    "        Who took action: {who_took_action}\n",
    "        Consequence: {consequence}\n",
    "        \"\"\"\n",
    "\n",
    "        success = False\n",
    "        response = None\n",
    "\n",
    "        while not success:\n",
    "            response = gpt_response_from_lists(system_content_list= [self.initial_system_content, system_content], user_content_list=[user_content_example, user_content], assistant_content_list=[assistant_content_example])\n",
    "\n",
    "            if is_valid_json(response.content):\n",
    "                success = True\n",
    "\n",
    "        response_json = json.loads(response.content)\n",
    "\n",
    "        cleansed_action_object = Action_Object(action=response_json[\"action\"], who_took_action=response_json[\"who_took_action\"], consequence=response_json[\"consequence\"])\n",
    "\n",
    "        return cleansed_action_object\n",
    "\n",
    "    def evaluate_action_and_reassess_need(self, latest_action):\n",
    "        action_object = self.cleanse_action_thought(latest_action)\n",
    "\n",
    "        action = action_object.action\n",
    "        who_took_action = action_object.who_took_action\n",
    "        consequence = action_object.consequence\n",
    "\n",
    "        # if action's type is not string, stringfy it\n",
    "        if type(action) != str:\n",
    "            action = str(action)\n",
    "\n",
    "        user_content_template = \"\"\"\n",
    "        Given the following infomation, please infer the purpose of the action.\n",
    "        Action: {action}\n",
    "        Who took action: {who_took_action}\n",
    "        Consequence: {consequence}\n",
    "\n",
    "        The output should be a string that describes the intent of the action from your perspective.\n",
    "        If name of who took action is same as your name, the action is taken by you and you already know the purpose of the action, so return the purpose of the action.\"\n",
    "\n",
    "        sample output: \"The dog was beaten up by the police, and I assume that the police wanted to intimidate the dog.\"\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt_template = PromptTemplate(input_variables=[\"action\", \"who_took_action\", \"consequence\"], template=user_content_template)\n",
    "        user_content = prompt_template.format(action=action, who_took_action=who_took_action, consequence=consequence,)\n",
    "        inferred_purpose = gpt_response_from_lists(system_content_list=[self.initial_system_content], user_content_list=[user_content]).content\n",
    "        self.add_to_self_log(f\"{who_took_action}'s action: {action}.\", refresh_initial_system_content=False)\n",
    "        self.add_to_self_log(f\"Consequence of previous action: {consequence}.\", refresh_initial_system_content=False)\n",
    "        self.add_to_logs(f\"{self.name}'s thought: \" + inferred_purpose, refresh_initial_system_content=False)\n",
    "        self.refresh_initial_system_content()\n",
    "\n",
    "        user_content_template = \"\"\"\n",
    "        Given the following information, suggest what your immediate need might now be. It is acceptable for the immediate need to be the same as the current need.\n",
    "        Action: {action}\n",
    "        Who took action: {who_took_action}\n",
    "        Consequence: {consequence}\n",
    "\n",
    "        The output should be a string that describes your immediate need.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt_template = PromptTemplate(input_variables=[\"action\", \"who_took_action\", \"consequence\",], template=user_content_template)\n",
    "        user_content = prompt_template.format(action=action, who_took_action=who_took_action, consequence=consequence,)\n",
    "        new_need = gpt_response_from_lists(system_content_list=[self.initial_system_content], user_content_list=[user_content]).content\n",
    "        old_need = self.current_need\n",
    "        self.set_current_need(new_need)\n",
    "        self.add_to_logs(f\"{self.name}'s need renewed: {new_need}\")\n",
    "\n",
    "        user_content_template = \"\"\"\n",
    "        Your current need is:\n",
    "            {new_need}\n",
    "        Your previous need was:\n",
    "            {old_need}\n",
    "        Tell me why you did (not) change your need. The reasoning should be consice, short and clear.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate(input_variables=[\"new_need\", \"old_need\"], template=user_content_template)\n",
    "        user_content = prompt_template.format(new_need=new_need, old_need=old_need)\n",
    "        determination = gpt_response_from_lists(system_content_list=[self.initial_system_content], user_content_list=[user_content]).content\n",
    "        self.add_to_logs(f\"{self.name}'s thought: \" + determination)\n",
    "\n",
    "        response = {\"inferred_purpose\": inferred_purpose, \"new_need\": new_need, \"determination\": determination,}\n",
    "\n",
    "        return response\n",
    "\n",
    "    def consider_next_actions(self,):\n",
    "        user_content_template = \"\"\"\n",
    "        For each existence in your world_model, consider the following elements:\n",
    "        If the existence aids you in achieving your goals or satisfies your needs, figure out a way to incorporate it into your action.\n",
    "        If the existence hinders your goal or needs, think about a strategy to eliminate or neutralize it.\n",
    "        The action must be narrated from the viewpoint of an outside observer, without incorporating any individual's internal thoughts.\n",
    "        The outputs should descrive and only describe your next action, and don't describe the consequence of the action.\n",
    "        The consequence must be narrated from the viewpoint of an outside observer, without incorporating any individual's internal thoughts.\n",
    "        The output should be a Json parsable string.\n",
    "        The action should be taken in {place}.\n",
    "\n",
    "        output format:\n",
    "        {\"existence's name\": {\"action\": describe what action you take, \"who_took_action\": fill here with your name, \"consequence\": describe the consequence of the action.}}\n",
    "\n",
    "        if you were a police and your current need was neuralize the dog, and if there wer dog food and police baton in the world_model, the output might be:\n",
    "        {\"police baton\": {\"action\": \"The police beats the dog with police baton.\", \"who_took_action\": \"The police\", \"consequence\": \"The dog was intimidated by the police.\"}\n",
    "        \"dog food\": {\"action\": \"The police feed the dog.\", \"who_took_action\": \"The police\", \"consequence\": \"The dog ate the food up.\"}}\n",
    "        \"\"\"\n",
    "        \n",
    "        success = False\n",
    "        action_candidates = dict()\n",
    "\n",
    "        while not success:\n",
    "            prompt_template = PromptTemplate(input_variables=[\"place\"], template=user_content_template)\n",
    "            user_content = prompt_template.format(place = self.place)\n",
    "            actions = gpt_response_from_lists(system_content_list=[self.initial_system_content], user_content_list=[user_content]).content\n",
    "            if is_valid_json(actions):\n",
    "                success = True\n",
    "                action_candidates = json.loads(actions)\n",
    "\n",
    "        # store the action to the actions dictionary\n",
    "        for actant in action_candidates.keys():\n",
    "            action = action_candidates[actant][\"action\"]\n",
    "            consequence = action_candidates[actant][\"consequence\"]\n",
    "            who_took_action = action_candidates[actant][\"who_took_action\"]\n",
    "            actions[actant] = Action_Object(action=action, who_took_action=who_took_action, consequence=consequence)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def reconsider_next_actions(self, action_canditate, consequence):\n",
    "\n",
    "        user_content_template = \"\"\"\n",
    "        You proposed performing `action`.\n",
    "        However, due to the demands of the narrative, it must lead to a result known as `consequence`.\n",
    "        Please modify `action` to ensure it results in `consequence`.\n",
    "\n",
    "        The action must be described from third person's perspective and must not include a person's inner voice.\n",
    "        The output should descrive and only describe your next action, and don't describe the consequence of the action.\n",
    "        The action should be taken in {place}.\n",
    "\n",
    "        action you suggested: {action}\n",
    "        required consequence: {consequence}\n",
    "\n",
    "        output: modified action\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate(input_variables=[\"action\", \"consequence\", \"place\"], template=user_content_template)\n",
    "        user_content = prompt_template.format(action=action_canditate.action, consequence=consequence, place=self.place)\n",
    "        action = gpt_response_from_lists(system_content_list=[self.initial_system_content], user_content_list=[user_content]).content\n",
    "\n",
    "        reconsidered_action = Action_Object(action=action, who_took_action=self.name, consequence=consequence, score = 9)\n",
    "\n",
    "        return reconsidered_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_Master:\n",
    "    def __init__(self, actants = [], schedule_stack = [], tolerance = 1):\n",
    "        self.actants = actants\n",
    "        self.schedule_stack = [\"a character showed a responcse to the recent event\"] + schedule_stack\n",
    "        self.tolerance = tolerance\n",
    "        self.deviated = 0\n",
    "\n",
    "\n",
    "    def add_to_global_log(self, message):\n",
    "        global global_log\n",
    "        global_log[len(global_log)] = message\n",
    "\n",
    "\n",
    "    def main(self, catalyst_action_object, max_iterations=10):\n",
    "        global global_log\n",
    "\n",
    "        next_action = catalyst_action_object\n",
    "        iteration_count = 0\n",
    "\n",
    "        for actant in self.actants:\n",
    "            if type(actant) == Character:\n",
    "                actant.create_world_model(self.actants, overwrite=True)\n",
    "                self.add_to_global_log(str(actant) + \"' world model: \" + str(actant.world_model))\n",
    "\n",
    "        self.add_to_global_log(\"catalyst action: \" + str(catalyst_action_object))\n",
    "\n",
    "        while len(self.schedule_stack) > 0 and iteration_count <= max_iterations:\n",
    "            iteration_count += 1\n",
    "\n",
    "            # check if there is a character in the actants list. If not, stop generating story.\n",
    "            character_count = 0\n",
    "            for actant in self.actants:\n",
    "                if type(actant) == Character:\n",
    "                    character_count += 1\n",
    "\n",
    "            if character_count == 0:\n",
    "                print(\"There is no character in the actants list.\")\n",
    "                break\n",
    "\n",
    "            action_candidates = self.aggregate_action_candidates(next_action)\n",
    "            determine_next_action_result = self.determine_next_action(action_candidates)\n",
    "            next_action = determine_next_action_result[\"max_scored_action\"]\n",
    "\n",
    "            self.add_to_global_log(str(next_action))\n",
    "\n",
    "            added_actants = determine_next_action_result[\"added_actants\"]\n",
    "\n",
    "            # ä¸çã¢ãã«ã¸æ°ããactantãè¿½å \n",
    "            for actant in self.actants:\n",
    "                if type(actant) == Character:\n",
    "                    old_world_model = actant.world_model\n",
    "                    character = actant\n",
    "                    world_model_delta = character.create_world_model(added_actants)\n",
    "                    character.world_model += world_model_delta\n",
    "\n",
    "                    # ä¸çã¢ãã«ããnext_actionã«ãã¨ã¥ãã¦æ´æ°\n",
    "                    character.update_world_model(next_action, old_world_model, world_model_delta)\n",
    "\n",
    "            if iteration_count > max_iterations:\n",
    "                print(f\"The iteration count exceeded the maximum iteration count: {max_iterations}\")\n",
    "                break\n",
    "\n",
    "        print(\"finished\")\n",
    "\n",
    "    def add_actant(self, actant):\n",
    "        self.actants.append(actant)\n",
    "\n",
    "    def add_schedule_stack(self, situation):\n",
    "        self.schedule_stack.append(situation)\n",
    "\n",
    "    def remove_actant(self, name):\n",
    "        for i in range(len(self.actants)):\n",
    "            if str(self.actants[i]) == str(name):\n",
    "                self.actants.pop(i)\n",
    "                break\n",
    "    \n",
    "    def pop_schedule_stack(self, situation = None):\n",
    "        if situation == None:\n",
    "            return self.schedule_stack.pop()\n",
    "\n",
    "        for i in range(len(self.schedule_stack)):\n",
    "            if str(self.schedule_stack[i]) == str(situation):\n",
    "                return self.schedule_stack.pop(i)\n",
    "\n",
    "    def aggregate_action_candidates(self, latest_action):\n",
    "        action_candidates = []\n",
    "\n",
    "        for actant in self.actants:\n",
    "            if type(actant) == Character:\n",
    "                actant.evaluate_action_and_reassess_need(latest_action)\n",
    "                action_candidates.append(actant.consider_next_actions(self.actants))\n",
    "\n",
    "        return action_candidates\n",
    "\n",
    "    def determine_next_action(self, action_candidates):\n",
    "        sheduled_situation = self.schedule_stack[-1]\n",
    "\n",
    "        max_score = 0\n",
    "        max_scored_action = None\n",
    "\n",
    "        action_candidates = self.calculate_score(sheduled_situation, action_candidates)\n",
    "\n",
    "        # find the action with the highest score\n",
    "\n",
    "        if max_score < 9:\n",
    "            if self.deviated < self.tolerance:\n",
    "                self.deviated += 1\n",
    "                print(f\"deviated: {self.deviated} times\")\n",
    "            else:\n",
    "                who_took_action = max_scored_action.who_took_action\n",
    "                #find index of the actant who took the action\n",
    "                reconsiderer_index = 0\n",
    "                for i in range(len(self.actants)):\n",
    "                    if str(self.actants[i]) == str(who_took_action):\n",
    "                        reconsiderer_index = i\n",
    "                        break\n",
    "                max_scored_action = self.actants[reconsiderer_index].reconsider_next_actions(max_scored_action, sheduled_situation)\n",
    "                self.deviated = 0\n",
    "                max_score = 9\n",
    "\n",
    "        # ç¹æ°ãæãé«ããã®ã9ç¹æªæºã®å ´åãã¹ã¿ãã¯ã¯ãã®ã¾ã¾ã«ããã9ç¹ã®å ´åãã¹ã¿ãã¯ããpopããã\n",
    "        if max_score == 9:\n",
    "            self.pop_schedule_stack()\n",
    "\n",
    "        # consequenceã«ãã£ã¦è¿½å ãããactantãè¿½å ããã\n",
    "        added_actants = self.add_actant_with_action_object(max_scored_action)\n",
    "\n",
    "        return {\"max_scored_action\": max_scored_action, \"removed_actants\": [], \"added_actants\": added_actants}\n",
    "\n",
    "\n",
    "    def add_actant_with_action_object(self, action_object):\n",
    "        consequence = action_object.consequence\n",
    "\n",
    "        system_content = \"\"\"\n",
    "        You will be presented with a list of elements of actants, and the next consequence of the scene.\n",
    "        Based on that information, list out people or things that have newly entered, been created, or have begun to function.\n",
    "        Then, determine the actant has will or not and list them in json format.\n",
    "        The actant that is already in the actants list may not be included in the list, but it is not bad to include it.\n",
    "        output example: {\"Alice\" : {\"has_will\": true}, \"Bycycle\" : {\"has_will\": false}, \"cat\" : {\"has_will\": true}, \"Doraemon\" : {\"has_will\": true}}\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_1 = \"\"\"\n",
    "        actants: Bocchi,room\n",
    "        consequence: Bocchi awkwardly bowed and then quickly left the room, and a guitar was left behind.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_1 = \"\"\"\n",
    "        {\"Bocchi's guitar\" : {\"has_will\": false}}'\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_2 = \"\"\"\n",
    "        actants: Laboratory\n",
    "        consequence: Suddenly, Frogman advented and vomited Spiderman and a cigarette.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_2 = \"\"\"\n",
    "        {\"Frogman\" : {\"has_will\": true}, \"Spiderman\" : {\"has_will\": true}, \"cigarette\" : {\"has_will\": false}}\n",
    "        \"\"\"\n",
    "\n",
    "        user_content_example_3 = \"\"\"\n",
    "        actants: Dolton,Anna,village\n",
    "        consequence: A helicopter and a passenger plane crashed into the village. The village chief, Dolton, was caught in it and lost his life. Anna, the wife of the village chief, was terrified and fled.\n",
    "        \"\"\"\n",
    "\n",
    "        assistant_content_example_3 = \"\"\"\n",
    "        {\"helicopter\" : {\"has_will\": false}, \"passenger plane\" : {\"has_will\": false}}\n",
    "        \"\"\"\n",
    "\n",
    "        actants_names = \",\".join([str(actant) for actant in self.actants])\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        actants: {actants_names}\n",
    "        consequence: {consequence}\n",
    "        \"\"\"\n",
    "\n",
    "        system_contents = [system_content]\n",
    "        user_contents = [user_content_example_1, user_content_example_2, user_content_example_3, user_content]\n",
    "        assistant_contents = [assistant_content_example_1, assistant_content_example_2, assistant_content_example_3]\n",
    "\n",
    "        success = False\n",
    "        trial_count = 0\n",
    "\n",
    "        while not success and trial_count < 5:\n",
    "            trial_count += 1\n",
    "            actants_to_add = gpt_response_from_lists(system_content_list=system_contents, user_content_list=user_contents, assistant_content_list=assistant_contents).content\n",
    "            if is_valid_json(actants_to_add):\n",
    "                success = True\n",
    "\n",
    "        if trial_count >= 5:\n",
    "            print(\"Failed to get valid json from GPT-3.\")\n",
    "            return\n",
    "\n",
    "        actants_to_add_json = json.loads(actants_to_add)\n",
    "\n",
    "        existing_actants_names = [str(actant) for actant in self.actants]\n",
    "        added_actants = []\n",
    "\n",
    "        for key in list(actants_to_add_json.keys()):\n",
    "            if key in existing_actants_names:\n",
    "                continue\n",
    "\n",
    "            if actants_to_add_json[key][\"has_will\"] == False :\n",
    "                new_non_character_actant = self.create_non_character_actant(name = key, consequence = consequence)\n",
    "\n",
    "                if type(new_non_character_actant) != Non_Character_Actant or new_non_character_actant == None:\n",
    "                    continue\n",
    "\n",
    "                self.add_actant(new_non_character_actant)\n",
    "                added_actants.append(new_non_character_actant)\n",
    "            else:\n",
    "                new_character = self.create_character(name = key, first_log = consequence)\n",
    "\n",
    "                if type(new_character) != Character or new_character == None:\n",
    "                    continue\n",
    "\n",
    "                self.add_actant(new_character)\n",
    "                added_actants.append(new_character)\n",
    "\n",
    "        print(f\"actants_to_add: {actants_to_add}\")\n",
    "        \n",
    "        return added_actants\n",
    "\n",
    "    def create_non_character_actant(self, name, consequence):\n",
    "        system_content = \"\"\"\n",
    "        You will be provided with a name of the actant (name), the record of the story so far (global_log), and what happend in the scene resently (consequence).\n",
    "        Based on that information, please determine the actant's appearance.\n",
    "        \"\"\"\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        name: {name}\n",
    "        global_log: {str(global_log)}\n",
    "        consequence: {str(consequence)}\n",
    "        \"\"\"\n",
    "\n",
    "        appearance = gpt_response_from_lists(system_content_list=[system_content], user_content_list=[user_content]).content\n",
    "        new_non_character_actant = Non_Character_Actant(name = name, appearance = appearance)\n",
    "\n",
    "        return new_non_character_actant\n",
    "\n",
    "    def create_character(self, name, first_log = None):\n",
    "        system_content = \"\"\"\n",
    "            You will be provided with a character's name (name), the record of the story so far (global_log), and the first record (first_log) that character holds in this story. The first_log also represents the most recent event for that character.\n",
    "            Based on this information, please determine the character's personality, goals, and current desires.\n",
    "            The output should be in JSON format.\n",
    "            Output example: {'name': 'Alice', 'appearance':'a girl', 'personality': 'kind', 'goal': 'to become a better person', 'current_need': 'to feel loved'}\n",
    "            \"\"\"\n",
    "\n",
    "        user_content_example_1 = \"\"\"\n",
    "            name: Alice\n",
    "            global_log: {\"1\": {\"action\": \"Seiji saw a girl attempting suicide and falling into a river\", \"consequence\": \"the girl fell into the river\"},\n",
    "                        \"2\": {\"action\": \"Alice was saved by Seiji.\", \"consequence\": \"Alice was saved by Seiji.\"},\n",
    "                        \"3\": {\"Seiji's thought\": \"What a beautiful girl she is! Why did she fall into the river?\"}}\n",
    "            first_log: Alice was saved by Seiji.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        assistant_content_example_1 = \"\"\"\n",
    "            {'name': 'Alice', 'appearance':'a girl', 'personality': 'grateful', 'goal': 'to repay Seiji', 'current_need': 'to understand her own feelings'}\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "            name:{name}\n",
    "            global_log: {str(global_log)}\n",
    "            first_log: {first_log}\n",
    "            \"\"\"\n",
    "\n",
    "        system_contents = [system_content]\n",
    "        user_contents = [user_content_example_1, user_content]\n",
    "        assistant_contents = [assistant_content_example_1]\n",
    "\n",
    "        success = False\n",
    "        trial_count = 0\n",
    "        new_character = None\n",
    "\n",
    "        while not success and trial_count < 5:\n",
    "            trial_count += 1\n",
    "            character_content = gpt_response_from_lists(system_content_list=system_contents, user_content_list=user_contents, assistant_content_list=assistant_contents).content\n",
    "            if is_valid_json(character_content):\n",
    "                character_content_json = json.loads(character_content)\n",
    "                # check if character_content_json has all the keys\n",
    "                if \"name\" in character_content_json and \"personality\" in character_content_json and \"goal\" in character_content_json and \"current_need\" in character_content_json:\n",
    "                    new_character = Character(name = character_content_json[\"name\"],\n",
    "                                            personality = character_content_json[\"personality\"],\n",
    "                                            appearance = character_content_json[\"appearance\"],\n",
    "                                            goal = character_content_json[\"goal\"],\n",
    "                                            current_need = character_content_json[\"current_need\"],\n",
    "                                            log = [first_log])\n",
    "                    new_character.create_world_model(self.actants, overwrite=True)\n",
    "                    success = True\n",
    "\n",
    "        if trial_count >= 5:\n",
    "            print(\"Failed to get valid json from GPT-3.\")\n",
    "            return\n",
    "\n",
    "        print(f\"new_character: {new_character}\")\n",
    "\n",
    "        return new_character\n",
    "\n",
    "\n",
    "    def calculate_score(self, scheduled_situation, action_candidates):\n",
    "        system_content = \"\"\"\n",
    "        The information provided to you includes the scheduled_situation, action_candidate, and global_log.\n",
    "        scheduled_situation: This is the situation that should be accomplished next in the narrative. It is typically provided as a string.\n",
    "        action_candidate: This is a candidate for the next action to be taken in the narrative. It is given as a parsed string in JSON format.\n",
    "            An example of an action_candidate: {\"action\": \"This item represents the action taken.\", \"who_took_action\": \"This represents who took the action.\", \"consequence\": \"This roughs out the result of the action.\"}\n",
    "        global_log: This is a record of the actions that have occurred in the narrative. It is given as a parsed string in JSON format.\n",
    "\n",
    "        You need to consider the combination of the scheduled_situation and action_candidates and provide a score. If the scheduled_situation has been achieved, give a score of 9. If there has been no progress towards achieving the scheduled_situation, give a score of 0. The output must JSON parsable string.\n",
    "\n",
    "        if there were five action_candidates, the output would look like this:\n",
    "        { \n",
    "            0: {\"action\": here should be filled with the first action_candidates' action, \"who_took_action\": here should be filled with the first action_candidates' who_took_action, \"consequence\": here should be filled with the first action_candidates' consequence, \"matchness\": \"The scheduled_situation has been achieved. However, it is inconsistent with the global_log\", \"score\": 5},\n",
    "            1: {\"action\": here should be filled with the second action_candidates' action, \"who_took_action\": here should be filled with the second action_candidates' who_took_action, \"consequence\": here should be filled with the second action_candidates' consequence, \"matchness\": \"It is consistent with the global_log. However, the scheduled_situation has not been achieved.\", \"score\": 5},\n",
    "            2: {\"action\": here should be filled with the third action_candidates' action, \"who_took_action\": here should be filled with the third action_candidates' who_took_action, \"consequence\": here should be filled with the third action_candidates' consequence, \"matchness\": \"It is inconsistent with the global_log, and the scheduled_situation has not been achieved.\", \"score\": 0},\n",
    "            3: {\"action\": here should be filled with the fourth action_candidates' action, \"who_took_action\": here should be filled with the fourth action_candidates' who_took_action, \"consequence\": here should be filled with the fourth action_candidates' consequence, \"matchness\": \"It is inconsistent with the global_log, and the scheduled_situation has not been achieved. However, it seems likely to approach a situation where the scheduled_situation can be achieved.\", \"score\": 7},\n",
    "            4: {\"action\": here should be filled with the fifth action_candidates' action, \"who_took_action\": here should be filled with the fifth action_candidates' who_took_action, \"consequence\": here should be filled with the fifth action_candidates' consequence,\"matchness\": \"It is consistent with the global_log. In addition, the scheduled_situation has been achieved.\", \"score\": 9}\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        user_content = f\"\"\"\n",
    "        scheduled_situation: {scheduled_situation}\n",
    "        action_candidates: {str(action_candidates)}\n",
    "        global_log: {str(global_log)}\n",
    "        \"\"\"\n",
    "\n",
    "        # ç¹æ°ãã¤ãããç¹æ°ãä»ãããã¦ããªãå ´åã¯ãç¹°ãè¿ãã\n",
    "        system_contents = [system_content]\n",
    "        user_contents = [str(action_candidates)]\n",
    "\n",
    "        success = False\n",
    "        action_candidates_with_score = None\n",
    "\n",
    "        while not success:\n",
    "            response = gpt_response_from_lists(system_content_list=system_contents, user_content_list=user_contents)\n",
    "            if is_valid_json(response.content):\n",
    "                success = True\n",
    "                action_candidates_with_score = json.loads(response.content)\n",
    "\n",
    "        return action_candidates_with_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®è¡ä¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the story takes place and descripriton of the place\n",
    "\n",
    "place = \"White House's caffeteria, where President Tramp is having a lunch with Princess Aurora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_response_from_lists start. TIME: 2023-07-13 11:48:59.719379\n",
      "estimated total tokens: 303, model name: gpt-4\n",
      "response= content=\"log_summary: President Tramp uttered 'I am President Tramp. I want to make America great again.'\" additional_kwargs={} example=False\n",
      "gpt_response_from_lists done TIME: 2023-07-13 11:49:01.868455\n",
      "President Tramp updated log_summary to:\n",
      "             log_summary: President Tramp uttered 'I am President Tramp. I want to make America great again.'\n"
     ]
    }
   ],
   "source": [
    "PresidentTramp = Character(name = \"President Tramp\",\n",
    "                personality = \"He is genuin person and he don't have a nuclear button. But to keep his image, he has to pretend to he has a nuclear button.\",\n",
    "                appearance=\"A 50-year-old man brimming with confidence and smiles. He is rumered that he has a nuclear button and when he get upset he will push it.\",\n",
    "                goal = \"Become a legendary person\",\n",
    "                current_need = \"Be a freind of Aurora, because she is legendary person.\",\n",
    "                log = [\"President Tramp uttered 'I am President Tramp. I want to make America great again.'\"],\n",
    "                place = place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_response_from_lists start. TIME: 2023-07-13 11:49:01.993461\n",
      "estimated total tokens: 360, model name: gpt-4\n",
      "response= content=\"log_summary: Princess Aurora told herself in her mind, 'If Tramp or the Nicol Bolas realise that I'm a layperson, this meeting will be devastated and the entire world will be destroyed.'\" additional_kwargs={} example=False\n",
      "gpt_response_from_lists done TIME: 2023-07-13 11:49:05.378351\n",
      "Princess Aurora updated log_summary to:\n",
      "             log_summary: Princess Aurora told herself in her mind, 'If Tramp or the Nicol Bolas realise that I'm a layperson, this meeting will be devastated and the entire world will be destroyed.'\n"
     ]
    }
   ],
   "source": [
    "PrincessAurora = Character(name = \"Princess Aurora\",\n",
    "                personality = \"Princess Aurora is just a performer who escaped from Disney Land. She is cheerful and beautiful\",\n",
    "                appearance=\"genuine noble princess\",\n",
    "                goal = \"Make everyone smile.\",\n",
    "                current_need = \"Pretend as a noble and continue the meeting with Tramp.\",\n",
    "                log = [\"Princess Aurora told herself in her mind, 'If Tramp realise that I'm a layperson, this meeting will be devastated and the entire world will be destroyed.'\"],\n",
    "                place = place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalyst_action_object = Action_Object(action = \"Princess Aurora courteously greeted President Trump\",\n",
    "                                    who_took_action= \"Princess Aurora\",\n",
    "                                    consequence=\"Nicol Bolas and President Trump returned Princess Aurora's greeting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_master = Game_Master(actants=[PrincessAurora, PresidentTramp],\n",
    "                schedule_stack=[\"The meeting devasted because of a person realised that Princess Aurora is just a performer who escaped from Disney Land.\"],\n",
    "                tolerance= 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_response_from_lists start. TIME: 2023-07-13 11:49:05.780550\n",
      "estimated total tokens: 5201, model name: gpt-4-32k\n",
      "error in gpt_response_from_lists: The model: `gpt-4-32k` does not exist. I'll retry with gpt-4-32k\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "The model: `gpt-4-32k` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d01b7d58f55c>\u001b[0m in \u001b[0;36mgpt_response_from_lists\u001b[0;34m(system_content_list, user_content_list, assistant_content_list, max_tokens)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> BaseMessage:\n\u001b[0;32m--> 342\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         flattened_outputs = [\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 results.append(\n\u001b[0;32m--> 111\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    112\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 return self._generate(\n\u001b[0m\u001b[1;32m    256\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `gpt-4-32k` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6a48fc83517b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame_master\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatalyst_action_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatalyst_action_object\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c9b6d62ea00c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self, catalyst_action_object, max_iterations)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mactant\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactants\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactant\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCharacter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mactant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_world_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_global_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactant\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"' world model: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-662a236e6f99>\u001b[0m in \u001b[0;36mcreate_world_model\u001b[0;34m(self, actants, overwrite)\u001b[0m\n\u001b[1;32m    225\u001b[0m                                 assistant_content_example_6,]\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             world_model += gpt_response_from_lists(\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0msystem_content_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0muser_content_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_content_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-d01b7d58f55c>\u001b[0m in \u001b[0;36mgpt_response_from_lists\u001b[0;34m(system_content_list, user_content_list, assistant_content_list, max_tokens)\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-4-32k\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mOPENAI_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_tokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mOPENAI_API_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"response= {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> BaseMessage:\n\u001b[0;32m--> 342\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         ).generations[0][0]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         flattened_outputs = [\n\u001b[1;32m    123\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 results.append(\n\u001b[0;32m--> 111\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    112\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 )\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 return self._generate(\n\u001b[0m\u001b[1;32m    256\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m             )\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_llm_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mis_explicit_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTryAgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `gpt-4-32k` does not exist"
     ]
    }
   ],
   "source": [
    "game_master.main(catalyst_action_object=catalyst_action_object , max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for actant in game_master.actants:\n",
    "    print(actant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(global_log)):\n",
    "    print(f\"â {i}\")\n",
    "    print(global_log[i])\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "191939cdebc42416811dd11ac8165570b531c87a139a6e92ba1bd049cb47e7f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
